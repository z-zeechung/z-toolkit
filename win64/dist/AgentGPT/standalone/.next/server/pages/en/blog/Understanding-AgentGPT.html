<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Reworkd Blog</title><meta name="description" content="Assemble, configure, and deploy autonomous AI Agents in your browser."/><meta name="twitter:site" content="@ReworkdAI"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Reworkd Blog"/><meta name="twitter:description" content="Assemble, configure, and deploy autonomous AI Agents in your browser."/><meta name="twitter:image" content="https://agentgpt.reworkd.ai/banner.png"/><meta name="twitter:image:width" content="1280"/><meta name="twitter:image:height" content="640"/><meta property="og:title" content="AgentGPT: Autonomous AI in your browser ðŸ¤–"/><meta property="og:description" content="Assemble, configure, and deploy autonomous AI Agents in your browser."/><meta property="og:url" content="https://agentgpt.reworkd.ai/"/><meta property="og:image" content="https://agentgpt.reworkd.ai/banner.png"/><meta property="og:image:width" content="1280"/><meta property="og:image:height" content="640"/><meta property="og:type" content="website"/><meta name="google-site-verification" content="sG4QDkC8g2oxKSopgJdIe2hQ_SaJDaEaBjwCXZNkNWA"/><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="mask-icon" href="/favicon.svg"/><meta name="next-head-count" content="21"/><link rel="preload" href="/_next/static/css/f53f09312f59da5a.css" as="style" crossorigin=""/><link rel="stylesheet" href="/_next/static/css/f53f09312f59da5a.css" crossorigin="" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" crossorigin="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-88c9e3382c92d13a.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/framework-f707628ec3f247aa.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/main-176c8907df556cb0.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/_app-4f89e4b7b4b87a41.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/1bfc9850-c1595fcb9b4862d7.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/983-8f87173a297455ab.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/702-ec17733db481970a.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/686-c2ea3b31a4f30d54.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/447-6f4b9fb01897f13f.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-16a5f839a36493a1.js" defer="" crossorigin=""></script><script src="/_next/static/klp32T_lMBqz5AWL-jk-X/_buildManifest.js" defer="" crossorigin=""></script><script src="/_next/static/klp32T_lMBqz5AWL-jk-X/_ssgManifest.js" defer="" crossorigin=""></script></head><body><div id="__next"><div><div class="min-w-screen mx-6 grid min-h-screen place-items-center py-2 selection:bg-purple-700/25 lg:overflow-x-hidden lg:overflow-y-hidden"><div class="flex h-full w-full max-w-[1440px] flex-col justify-between overflow-hidden"><div style="opacity:0;transform:translateX(0px) translateY(-30px) translateZ(0)"><nav class="z-50 w-full bg-transparent text-white" data-headlessui-state=""><div class="align-center flex h-16 flex-row justify-between"><div class="flex flex-shrink-0 cursor-pointer items-center lg:flex-1"><img alt="Reworkd AI" loading="lazy" width="25" height="25" decoding="async" data-nimg="1" class="mb-1 mr-2 invert-0" style="color:transparent" src="/logos/dark-default-solid.svg"/><span class="text-xl font-light tracking-wider">Reworkd</span></div><div class="hidden flex-1 items-center justify-center xmd:flex"><div class="border-gradient flex h-[42px] items-center self-center overflow-hidden rounded-full bg-opacity-5 px-2 py-1 backdrop-blur-lg"><div class="relative inline-flex items-center justify-center"><div class="opacity-75 absolute -inset-1 rounded-full bg-gradient-to-tr from-[#A02BFE] to-[#1152FA] blur-lg transition-all duration-1000"></div><div class="relative z-10"><div class="flex h-[28px] w-[28px] flex-row justify-start gap-x-4 overflow-hidden rounded-full bg-white p-1.5"><div class="flex gap-2"><div><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none"><path fill="url(#icon-home_svg__a)" fill-rule="evenodd" d="M12 15H4a3 3 0 0 1-3-3V7.898a4 4 0 0 1 1.343-2.99l3.455-3.07a3.315 3.315 0 0 1 4.404 0l3.456 3.07A4 4 0 0 1 15 7.898V12a3 3 0 0 1-3 3ZM8 3c-.322 0-.633.118-.874.332L3.671 6.403A2 2 0 0 0 3 7.898V12a1 1 0 0 0 1 1h8a1 1 0 0 0 1-1V7.898a2 2 0 0 0-.671-1.495l-3.455-3.07A1.315 1.315 0 0 0 8 3Z" clip-rule="evenodd"></path><path fill="#000" d="M6 10.75a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 .75.75V13H6v-2.25Z"></path><defs><radialGradient id="icon-home_svg__a" cx="0" cy="0" r="1" gradientTransform="rotate(67.166 -2.392 5.386) scale(10.3078)" gradientUnits="userSpaceOnUse"><stop stop-opacity="0.3"></stop><stop offset="1"></stop></radialGradient></defs></svg></div><div><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none"><path fill="url(#icon-blogs_svg__a)" fill-rule="evenodd" d="M9 2.586A2 2 0 0 1 10.414 2h1.836c.966 0 1.75.784 1.75 1.75v8.5A1.75 1.75 0 0 1 12.25 14h-1.836A2 2 0 0 1 9 13.414L6.586 11H2.25C1.56 11 1 10.44 1 9.75v-3.5C1 5.56 1.56 5 2.25 5h4.336L9 2.586ZM12 4h-1.586l-3 3H3v2h4.414l3 3H12V4Z" clip-rule="evenodd"></path><path fill="#000" fill-opacity="0.99" d="M3 11h3v2a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1v-2Z"></path><path fill="#000" d="M14 6a1 1 0 0 1 1 1v2a1 1 0 0 1-1 1V6Z"></path><defs><radialGradient id="icon-blogs_svg__a" cx="0" cy="0" r="1" gradientTransform="matrix(7.5 1.5 -2.52573 12.6287 2 8)" gradientUnits="userSpaceOnUse"><stop stop-opacity="0.3"></stop><stop offset="1"></stop></radialGradient></defs></svg></div><div><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none"><path fill="url(#icon-pricing_svg__a)" fill-rule="evenodd" d="M13.25 14H2.75A2.75 2.75 0 0 1 0 11.25v-6.5A2.75 2.75 0 0 1 2.75 2h10.5A2.75 2.75 0 0 1 16 4.75v6.5A2.75 2.75 0 0 1 13.25 14ZM14 4.75a.75.75 0 0 0-.75-.75H2.75a.75.75 0 0 0-.75.75v6.5c0 .414.336.75.75.75h10.5a.75.75 0 0 0 .75-.75v-6.5Z" clip-rule="evenodd"></path><path fill="#000" d="M2 7h1v3H2V7ZM4 7h2v3H4V7ZM8 7H7v3h1V7ZM9 7h3v3H9V7ZM14 7h-1v3h1V7Z"></path><defs><radialGradient id="icon-pricing_svg__a" cx="0" cy="0" r="1" gradientTransform="matrix(10.69999 2 -2 10.69999 .8 5)" gradientUnits="userSpaceOnUse"><stop stop-opacity="0.3"></stop><stop offset="1"></stop></radialGradient></defs></svg></div><div><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none"><g clip-path="url(#icon-github_svg__a)"><path fill="url(#icon-github_svg__b)" d="M8 0c4.42 0 8 3.58 8 8a8.012 8.012 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z"></path></g><defs><radialGradient id="icon-github_svg__b" cx="0" cy="0" r="1" gradientTransform="rotate(39.701 -2.114 1.546) scale(16.8965)" gradientUnits="userSpaceOnUse"><stop stop-opacity="0.3"></stop><stop offset="1"></stop></radialGradient><clipPath id="icon-github_svg__a"><path fill="#fff" d="M0 0h16v16H0z"></path></clipPath></defs></svg></div><div><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none"><path fill="url(#icon-docs_svg__a)" fill-rule="evenodd" d="M12.25 15h-7.5A2.75 2.75 0 0 1 2 12.25v-9.5A2.75 2.75 0 0 1 4.75 0h4.37a4 4 0 0 1 2.949 1.297l1.88 2.05A4 4 0 0 1 15 6.052v6.199A2.75 2.75 0 0 1 12.25 15ZM13 6.05a2 2 0 0 0-.526-1.35l-1.88-2.051A2 2 0 0 0 9.12 2H4.75a.75.75 0 0 0-.75.75v9.5c0 .414.336.75.75.75h7.5a.75.75 0 0 0 .75-.75v-6.2Z" clip-rule="evenodd"></path><path fill="#000" d="M6 9.5a.5.5 0 0 1 .5-.5h4a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-4a.5.5 0 0 1-.5-.5v-1ZM6 5.5a.5.5 0 0 1 .5-.5h3a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1Z"></path><defs><radialGradient id="icon-docs_svg__a" cx="0" cy="0" r="1" gradientTransform="rotate(54.031 -2.352 4.474) scale(10.8113)" gradientUnits="userSpaceOnUse"><stop stop-opacity="0.3"></stop><stop offset="1"></stop></radialGradient></defs></svg></div></div></div></div></div><a href="/home" class="after-gradient relative flex flex-col items-center justify-center p-2 px-4 text-center font-inter text-sm tracking-normal text-white transition-colors duration-700 before:absolute before:-bottom-[20px] before:-z-20 before:h-6 before:w-12 before:bg-white/60 before:blur-lg before:transition-opacity before:duration-700 after:absolute after:-bottom-[2.25px] after:h-[1px] after:w-16 after:px-2 after:transition-opacity after:duration-700 hover:text-white text-white/50 before:opacity-0 after:opacity-0">Home</a><a href="/blog" class="after-gradient relative flex flex-col items-center justify-center p-2 px-4 text-center font-inter text-sm tracking-normal text-white transition-colors duration-700 before:absolute before:-bottom-[20px] before:-z-20 before:h-6 before:w-12 before:bg-white/60 before:blur-lg before:transition-opacity before:duration-700 after:absolute after:-bottom-[2.25px] after:h-[1px] after:w-16 after:px-2 after:transition-opacity after:duration-700 hover:text-white">Blog</a><a href="https://agentgpt.reworkd.ai/plan" class="after-gradient relative flex flex-col items-center justify-center p-2 px-4 text-center font-inter text-sm tracking-normal text-white transition-colors duration-700 before:absolute before:-bottom-[20px] before:-z-20 before:h-6 before:w-12 before:bg-white/60 before:blur-lg before:transition-opacity before:duration-700 after:absolute after:-bottom-[2.25px] after:h-[1px] after:w-16 after:px-2 after:transition-opacity after:duration-700 hover:text-white text-white/50 before:opacity-0 after:opacity-0">Pricing</a><a href="https://github.com/reworkd/AgentGPT" class="after-gradient relative flex flex-col items-center justify-center p-2 px-4 text-center font-inter text-sm tracking-normal text-white transition-colors duration-700 before:absolute before:-bottom-[20px] before:-z-20 before:h-6 before:w-12 before:bg-white/60 before:blur-lg before:transition-opacity before:duration-700 after:absolute after:-bottom-[2.25px] after:h-[1px] after:w-16 after:px-2 after:transition-opacity after:duration-700 hover:text-white text-white/50 before:opacity-0 after:opacity-0">Github</a><a href="https://docs.reworkd.ai/" class="after-gradient relative flex flex-col items-center justify-center p-2 px-4 text-center font-inter text-sm tracking-normal text-white transition-colors duration-700 before:absolute before:-bottom-[20px] before:-z-20 before:h-6 before:w-12 before:bg-white/60 before:blur-lg before:transition-opacity before:duration-700 after:absolute after:-bottom-[2.25px] after:h-[1px] after:w-16 after:px-2 after:transition-opacity after:duration-700 hover:text-white text-white/50 before:opacity-0 after:opacity-0">Docs</a></div></div><div class="hidden justify-end gap-2 xmd:flex sm:items-center lg:flex-1"><div class="relative inline-flex items-center justify-center"><div class="opacity-40 absolute -inset-1 rounded-full bg-gradient-to-tr from-[#A02BFE] to-[#1152FA] blur-lg transition-all duration-1000"></div><div class="relative z-10"><button class="group rounded-full border border-black bg-white text-black transition duration-300 ease-in-out hover:hover:bg-neutral-200 focus-visible:bg-white/90 focus-visible:outline-none focus-visible:ring-4 focus-visible:ring-white/30"><div class="flex items-center justify-center gap-x-2.5 px-4 py-1 font-inter text-sm leading-6"><span>Join the Waitlist</span><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 320 512" class="text-gray-700 transition-transform group-hover:translate-x-1" height="12" width="12" xmlns="http://www.w3.org/2000/svg"><path d="M285.476 272.971L91.132 467.314c-9.373 9.373-24.569 9.373-33.941 0l-22.667-22.667c-9.357-9.357-9.375-24.522-.04-33.901L188.505 256 34.484 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941 0L285.475 239.03c9.373 9.372 9.373 24.568.001 33.941z"></path></svg></div></button></div></div></div><div class="-mr-2 flex items-center xmd:hidden"><button class="inline-flex items-center justify-center rounded-md p-2 text-white hover:bg-neutral-700 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:ring-offset-2" id="headlessui-disclosure-button-:R26dm:" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" class="block h-6 w-6" aria-hidden="true" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"></path></svg></button></div></div></nav></div><div style="opacity:0;transform:translateX(0px) translateY(-30px) translateZ(0)"><div class="flex min-h-screen justify-center"><div class="bg-stars animate-stars"></div><div class="flex h-full max-w-[1440px] flex-col justify-between"><main class="mx-auto px-6 lg:px-8"><div class="bg-transparent py-8 sm:py-16"><div class="mx-auto max-w-2xl text-center"><h2 class="text-3xl font-bold tracking-tight text-white sm:text-4xl">Reblogd</h2></div></div></main><div class="mx-auto mb-8 max-w-2xl sm:mb-16"><div class="text-white"><p>July 17th, 2023</p><div class="prose text-white"><p><img src="https://petal-diplodocus-04a.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fef520689-ca1b-4489-98aa-41136f565840%2FCybrCo_Art_human-like_robot_typing_on_a_computer_in_a_dark_room_a0174b88-a5b9-4b82-98c6-734dbbde8d09.webp?id=f768fec9-bd6a-43ae-811d-1adb065c6c8e&amp;table=block&amp;spaceId=46c3481b-d8de-4c34-8647-2292d63a5f29&amp;width=2000&amp;userId=&amp;cache=v2" alt="CybrCo_Art_human-like_robot_typing_on_a_computer_in_a_dark_room_a0174b88-a5b9-4b82-98c6-734dbbde8d09.webp"/></p>
<blockquote>
<p>Alt: A robotic agent types at a laptop in a dark room.</p>
</blockquote>
<hr/>
<p>The invention of the <strong>Generative Pre-trained Transformer (GPT)</strong> is one of the recent decade&#x27;s most important
advancements in AI technology. The GPTs powering today&#x27;s <strong>Large Language Models (LLMs)</strong> demonstrate a <em>remarkable
ability for reasoning, understanding, and planning</em>. However, their true potential has yet to be fully realized.</p>
<p>At <strong>Reworkd</strong>, we believe that the <em>true power of LLMs lies in agentic behavior</em>. By engineering a system that draws on
LLMs&#x27; emergent abilities and providing an ecosystem that supports environmental interactions, we can draw out the full
potential of models like GPT-4. Here&#x27;s how AgentGPT works.</p>
<h2>LLMs have a lot of limitations.</h2>
<p>The main products shipping LLMs are chatbots powered by</p>
<p><a href="https://www.techopedia.com/definition/34826/foundation-model">Foundation Model - Techopedia</a>.</p>
<p>If you have any familiarity working with OpenAI&#x27;s API, a common formula you might use for chatting with the model may
include:</p>
<ul>
<li>Taking the user&#x27;s message.</li>
<li>Adding a list of chat histories.</li>
<li>Sending the chat history across the API to retrieve a completion.</li>
</ul>
<p>This method works fine when the scope of conversations is small; however, <em>as you continue adding new messages to the
chat history, the size and complexity of completions balloons</em>, and you will quickly run into a wall: the dreaded
context limit.</p>
<p>A <strong>context limit</strong> is the maximum number of <strong>tokens</strong> (a token usually represents a single word) that can be input
into
the model for a single response. They are necessary because the <em>computational cost as we add additional tokens tends to
increase quadratically</em>. However, they are often the bane of prompt engineers.</p>
<p>One solution is to measure the number of tokens in the chat history before sending it to the model and removing old
messages to ensure it fits the token limit. While this approach works, it ultimately reduces the amount of knowledge
available to the assistant.</p>
<p>Another issue that standalone LLMs face is the need for human guidance. Fundamentally, LLMs are next-word predictors,
and often, their internal structure is not inherently suited to higher-order thought processes, such as <strong>reasoning</strong>
through complex tasks. This weakness doesn&#x27;t mean they can&#x27;t or don&#x27;t reason. In fact, there are several <a href="https://arxiv.org/abs/2205.11916#:~:text=While%20these%20successes%20are%20often%20attributed%20to%20LLMs%27,%22Let%27s%20think%20step%20by%20step%22%20before%20each%20answer.">studies</a> that shows they can. However, it does mean they face certain impediments. For example, the LLM itself can create a logical list of steps; however, it has <em>no built-in mechanisms for observation and reflection on that list.</em></p>
<p>A pre-trained model is essentially a &quot;black box&quot; for the end user in which the final product that is shipped has
<em>limited to no capability of actively updating its knowledge base and tends to act in unpredictable ways</em>. As a result,
it&#x27;s <a href="https://arxiv.org/abs/2202.03629">hallucination</a>-prone.</p>
<p>Thus, it requires a lot of effort on the user&#x27;s part to guide the model&#x27;s output, and prompting the LLM itself becomes a
job on its own. This extra work is a far cry from our vision of an AI-powered future.</p>
<p>By providing a platform to give LLMs agentic abilities, <em>AgentGPT aims to overcome the limitations of standalone LLMs by
leveraging prompt engineering techniques, vector databases, and API tooling.</em> Hereâ€™s some interesting work that is being
done with the agent concept:</p>
<p><a href="https://twitter.com/DrJimFan/status/1673006745067847683"><img src="https://platform.twitter.com/embed/Tweet.html?dnt=false&amp;embedId=twitter-widget-0&amp;frame=false&amp;hideCard=false&amp;hideThread=false&amp;id=1673006745067847683&amp;lang=en&amp;origin=https%3A%2F%2Fpublish.twitter.com%2F%3Fquery%3Dhttps3A2F2Ftwitter.com2FDrJimFan2Fstatus2F1673006745067847683%26widget%3DTweet&amp;theme=light&amp;widgetsVersion=82e1070%3A1619632193066&amp;width=550px" alt="Tweet by Dr. Jim Fan"/></a></p>
<blockquote>
<p>Alt: A Twitter post by Dr. Jim Fan</p>
</blockquote>
<h2>What are agents?</h2>
<p>In a general sense, <a href="https://zapier.com/blog/ai-agent/">agents</a> are rational actors. They use thinking and reasoning to
influence their environment. <em>This could be in the form of solving problems or pursuing specific goals. They might
interact with humans or utilize tools.</em> Ultimately, we can apply this concept to LLMs to instill more intelligent and
logical behavior.</p>
<p>In AgentGPT, large language models essentially function as the <strong>brain</strong> of each agent. As a result, we can produce
powerful agents by cleverly <em>manipulating the English language</em> and engineering a <em>framework that supports
interoperability between LLM completions and a diverse set of APIs</em>.</p>
<h3>Engineering this system consists of 3 parts.</h3>
<p><strong>Reasoning and Planning.</strong> If you were to simply take a general goal, such as &quot;build a scaling e-commerce platform,&quot;
and
give it to ChatGPT, you would likely get a response along the lines of &quot;As an AI language modelâ€¦.&quot; However, through
<strong>prompt engineering</strong>, we can get a model to <em>break down goals into digestible steps and reflect on them</em> with a method
called chain of thought prompting.</p>
<p><strong>Memory.</strong> When dealing with memory, we divide the problem into <strong>short-term</strong> and <strong>long-term</strong>. In managing
short-term
memory, we can use prompting techniques such as <em>few-shot prompting to steer LLM responses</em>. However, <em>cost and context
limits make it tricky to generate completions without limiting the breadth of information</em> a model can use to make
decisions.</p>
<p>Similarly, this issue also arises in <strong>long-term memory</strong> because it would be impossible to provide an appropriate
corpus
of writing to bridge the gap between GPT -4&#x27;s cutoff date, 2021, till today. By using vector databases, we attempt to
overcome this using specialized models for <em>information retrieval in high-dimensional vector spaces</em>.</p>
<p><strong>Tools</strong>. Another challenge in using LLMs as general actors is their confinement to text outputs. Again, we can use
prompt engineering techniques to solve this issue. We can generate predictable function calls from the LLM through
few-shot and chain-of-thought methods, utilizing API tools like <strong>Google Search</strong>, <strong>Hugging Face</strong>, <strong>Dall-E</strong>, etc. In
addition, we can use fine-tuned LLMs that only return responses in specialized formatting, like JSON. This is the
approach OpenAI took when they recently released the function calling feature for their API.</p>
<p>These three concepts have formed the backbone of multiple successful agent-based LLM platforms such
as <a href="https://github.com/microsoft/JARVIS">Microsoft Jarvis</a>, <a href="https://github.com/Significant-Gravitas/Auto-GPT">AutoGPT</a>, <a href="https://github.com/yoheinakajima/babyagi">BabyAGI</a>,
and of course, AgentGPT. With this brief overview in mind, let&#x27;s dive deeper into each component.</p>
<h2>How do we get agents to act intelligently?</h2>
<p><strong>Prompt engineering</strong> has become highly popularized, and it&#x27;s only natural given its ability to <em>increase the
reliability of LLM responses</em>, opening a wide avenue of potential applications for generative AI. AgentGPT&#x27;s ability to
think and reason is a result of novel prompting methods.</p>
<h3>A Brief Intro to Prompt Engineering</h3>
<p>Prompt engineering is a largely empirical field that aims to find methods to steer LLM responses by finding clever ways
to use the English language. <em>You can think of it like lawyering, where every nuance in the wording of a prompt counts.</em></p>
<p>These are the main concepts and building blocks for more advanced prompting techniques:</p>
<ol>
<li><strong>Zero-Shot</strong> involves sending the raw command directly to the LLM with little to no formatting.</li>
<li><strong>Few-Shot</strong> gives context for completions in the form of example responses.</li>
<li><strong>Chain-of-Thought</strong> guides the model in reasoning through generating and reasoning over a complex task.</li>
</ol>
<h3>How AgentGPT Uses Prompt Engineering</h3>
<p>AgentGPT uses an advanced form of chain-of-thought prompting called <strong>Plan-and-Solve</strong> to generate the steps you see
when
operating the agents.</p>
<p>Traditionally, chain-of-thought prompting utilized few-shot techniques to provide examples of a thinking and reasoning
process. However, as is becomes a theme, it becomes more costly as the complexity of a task increases because we will
need to provide more context.</p>
<p><strong>Plan-and-solve (PS):</strong> By virtue of being a zero-shot method, it provides a <em>prompting framework for LLM-guided
reasoning using &quot;trigger&quot; words</em>. These keywords trigger a reasoning response from the model.</p>
<p>We can expand on this concept by <em>modifying the prompt to extract important variables and steps to generate a final
response with a cohesive format</em>. This method allows us to parse the final response and display it for the end user as
well as feed sub-steps into future plan-and-solve prompts.</p>
<p><img src="https://petal-diplodocus-04a.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F29d8c98c-21e6-4991-992d-62d95fd40dba%2FScreen_Shot_2023-07-01_at_12.25.37_PM.png?id=021895a6-149a-4282-aa8e-6719e7d7c47a&amp;table=block&amp;spaceId=46c3481b-d8de-4c34-8647-2292d63a5f29&amp;width=2000&amp;userId=&amp;cache=v2" alt="Screen Shot 2023-07-01 at 12.25.37 PM.png"/></p>
<blockquote>
<p>Alt: Picture of Plan &amp; Solve</p>
</blockquote>
<p>While PS prompting helps evoke a reasoning response, it still misses a fundamental concept in reasoning, and that is
proper handling for reflection and action. <strong>Reflection</strong>is <em>fundamental for any agent because it must rationalize an
action, perform that action, and use feedback to adjust future actions.</em> Without it, the agent would be stateless and
unchanging.</p>
<p>AgentGPT uses a prompting framework called Reasoning and Acting (<a href="https://arxiv.org/pdf/2210.03629.pdf">ReAct</a>) to
expand on the capabilities of the Plan-and-Solve concept. <strong>ReAct</strong> aims to <em>enable a framework for the model to access
fresh knowledge through external knowledge bases and make observations of actions it has taken</em>. Using those
observations, the LLM can make educated decisions on the next set of steps to complete while performing actions to query
knowledge bases such as <strong>Google Search</strong> or <strong>Wikipedia API</strong>.</p>
<p>Prompt engineering is largely effective in resolving challenges in short-term memory as well as instilling the reasoning
behavior that you can see when AgentGPT is at work. However, prompt engineering does not resolve the issue of long-term
memory. This issue is where vector databases come in, and we will look at those next.</p>
<p><img src="https://petal-diplodocus-04a.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F481f0812-00e5-4cb1-9ed6-4f2f9215eef5%2FScreen_Shot_2023-07-03_at_3.12.56_AM.png?id=8002f409-2913-4e68-b8b6-6100c4128cf5&amp;table=block&amp;spaceId=46c3481b-d8de-4c34-8647-2292d63a5f29&amp;width=2000&amp;userId=&amp;cache=v2" alt="Screen Shot 2023-07-03 at 3.12.56 AM.png"/></p>
<blockquote>
<p>Alt : ReAct (Reason + Act) Logic Picture</p>
</blockquote>
<blockquote>
<p>The ReAct framework allows us to generate a reasoning response, an action, and a reflection to
steer the modelâ€™s response. This example is courtesy of the following
paper: <a href="https://arxiv.org/abs/2210.03629">ReAct: Synergizing Reasoning and Acting in Language Models</a>*</p>
</blockquote>
<h2>How do we give agents a working memory?</h2>
<p>While we have seen that <em>prompt engineering is largely effective in resolving issues with short-term memory and
reasoning</em>, we cannot solve long-term memory solely through clever English. Since we are not allowed to update the model
to learn our data, we must build an external system for storing and retrieving knowledge.</p>
<p>A clever solution might use an LLM to <em>generate summaries of previous conversations as context for the prompt</em>. However,
there are three significant issues with this. First, we are diluting the relevant information for the conversation;
second, it introduces another cost area by paying for API usage for those summaries; and third, it&#x27;s unscalable.</p>
<p>Thus, prompts appear to be ineffective for long-term memory. Seeing as <em>long-term memory is a problem of storage and
efficient retrieval of information</em>, there is no absence of research in the study of search, so we must look towards
vector databases.</p>
<h3>Vector Databases Demystified</h3>
<p><strong><a href="https://aws.amazon.com/what-is/vector-databases/">Vector databases</a></strong> have been hyped up for a while now, and the
hype
is very deserved. They are an efficient way of storing and retrieving vectors by allowing us to use some fun new
<em>algorithms to query billions - even trillions - of data records in milliseconds.</em></p>
<p>Let&#x27;s start with a little bit of vocabulary:</p>
<ul>
<li>A <strong>vector</strong> in the context of an LLM is a representation of a piece of text that a model like GPT-4 encodes.</li>
<li>A <strong>vector space</strong> contains many of these vectors.</li>
<li>An <strong>embedding</strong> is the vectorized version of a text.</li>
</ul>
<h3>Vector libraries like</h3>
<p><a href="https://www.bing.com/ck/a?!&amp;&amp;p=a0f4167bc6cd7db9JmltdHM9MTY4ODM0MjQwMCZpZ3VpZD0zOTYwYjczZS1hNzg2LTY5Y2MtMjM2YS1hNDdmYTYwMjY4MjImaW5zaWQ9NTIwMQ&amp;ptn=3&amp;hsh=3&amp;fclid=3960b73e-a786-69cc-236a-a47fa6026822&amp;psq=faiss+github&amp;u=a1aHR0cHM6Ly9naXRodWIuY29tL2ZhY2Vib29rcmVzZWFyY2gvZmFpc3M&amp;ntb=1">Facebook AI Similarity Search</a> (
FAISS) give us access to valuable <em>tools to control these vectors and locate them efficiently in the vector space.</em></p>
<p>Since the text is in a numerical embedding dictated by the model type (i.e., text-embedding-ada-002), there is some
location in space that the text exists in, and it&#x27;s based on the numbers that compose its vector. That means <em>similar
texts will be represented as vectors with similar numbers, and thus, they will likely be grouped closely. On the other
hand, less similar texts will be further away</em>. For example, texts about cooking will be closer to food than texts about
physics.</p>
<p>There are several different algorithms for querying the vector space, but the most relevant to this discussion is the
cosine similarity search. <strong><a href="https://www.geeksforgeeks.org/cosine-similarity/">Cosine similarity</a></strong> measures the cosine
of the angle between two non-zero vectors. <em>It is a measure of orientation, meaning that it&#x27;s used to determine how
similar two documents (or whatever the vectors represent) are</em>. Cosine similarity can range from -1 to 1, with -1
meaning the vectors are diametrically opposed (completely opposite), 0 meaning the vectors are orthogonal (or
unrelated), and 1 meaning the vectors are identical.</p>
<p>FAISS is helpful in managing these vector spaces, but it is not a database. <em>Vector libraries
lack <a href="https://www.freecodecamp.org/news/crud-operations-explained/">CRUD</a> operations, which makes them alone unviable
for long-term memory</em>, and that&#x27;s where cloud services such as Pinecone and Weaviate step in.</p>
<p><strong>Pinecone</strong> and <strong>Weaviate</strong> essentially do all the hard work of managing our vectors. They provide an API that allows
you
to upload embeddings, perform various types of searches, and store those vectors for later. <em>They provide the typical
CRUD functions we need to instill memory into LLMs in easily-accessible Python modules.</em></p>
<p>By using them, we can encode large amounts of information for future storage and retrieval. For instance, when the LLM
needs extra knowledge to complete a task, we can prompt it to query the vector space to find relevant information. Thus,
we can create long-term memory.</p>
<p><img src="https://petal-diplodocus-04a.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fad2521b3-1c6b-4f16-b719-d2b766570c61%2FCybrCo_Art_A_human-like_robot_touching_a_flower_for_the_first_t_92e97d56-54fa-4bb0-8581-5a1e15fd94aa.webp?id=8d261d10-f4e4-4798-bc33-8f40da67bb42&amp;table=block&amp;spaceId=46c3481b-d8de-4c34-8647-2292d63a5f29&amp;width=2000&amp;userId=&amp;cache=v2" alt="CybrCo_Art_A_human-like_robot_touching_a_flower_for_the_first_t_92e97d56-54fa-4bb0-8581-5a1e15fd94aa.webp"/></p>
<blockquote>
<p>Alt : Robot With A Rose In Hand</p>
</blockquote>
<h2>Tools to interact with the environment</h2>
<p>While <strong>prompt engineering</strong> and <strong>vector databases</strong> resolve many of the limitations and challenges of LLMs, there is
still the problem of agent interaction. <em>How can we extend the capabilities of an LLM to interact with the environment
outside of text?</em></p>
<p>APIs are the answer. By utilizing APIs, we can give our agents the ability to perform a wide range of actions and
access external resources.</p>
<p>Here are a few examples:</p>
<ul>
<li><strong>Google Search API</strong>: Allows agents to search the web and retrieve relevant information.</li>
<li><strong>Hugging Face</strong>: Provides access to various NLP models and transformers for tasks such as summarization, translation,
sentiment analysis, and more.</li>
<li><strong>Dall-E</strong>: Enables agents to generate images from textual descriptions.</li>
<li><strong>OpenAI&#x27;s GPT API</strong>: Allows agents to utilize the GPT-4 model for text completion and generation.</li>
</ul>
<p>Using API tools in combination with prompt engineering techniques, we can create prompts that generate predictable
function calls and utilize the output of API requests to enhance the agent&#x27;s capabilities. This enables agents to
interact with the environment in a meaningful way beyond text-based interactions.</p>
<h3>Engineering Robust Function Calls</h3>
<p>Again, we can achieve tooling through prompt engineering by <em>representing the tool we want to provide for the model</em> as a <strong>function</strong>. <em>We can then tell the model that this function exists in a prompt, so our program can call it programmatically based on the model&#x27;s response</em>. First, however, we should examine the main challenges in implementing tool interactions: consistency, context, and format.</p>
<p>For example, responses tend to vary among chat completions that use the same prompt. Thus, getting the LLM to issue a function call consistently is challenging. A minor solution may include adjusting the <strong>temperature</strong> of the model (a parameter to control the randomness), but the best solution should leverage an LLM&#x27;s reasoning abilities. Thus, <em>we can use the ReAct framework to help the llm understand when to issue function calls.</em></p>
<p>In doing this, we will still run into another major issue. How will the LLMs understand what tools are at their disposal? We could include the available tools in a prompt, but this could significantly increase the number of tokens we would need to send to the model. While this may be fine for an application that runs on a couple of tools, it will increase costs as we add more tools to the system. Thus, <em>we would use vector databases to help the LLM look up relevant tools it needs.</em></p>
<p>Finally, we need to generate function calls in a predictable format. This format should include provisions for the name of the function and the parameters it takes, and it must include delimiters that allow us to parse and execute the response for those parameters programmatically. <em>For instance, you can prompt the model to only return responses in JSON and then use built-in Python libraries to parse the stringified JSON.</em></p>
<p>Recently, it became even easier to use this type of method as well. In late June, OpenAI released <strong>gpt-4-0613</strong> and <strong>gpt-3.5-turbo-16k-0613</strong> (whew, these names are getting long). They natively support function calls by using a model fine-tuned for JSON to return easy-to-use function calls. You can read more about it <a href="https://platform.openai.com/docs/guides/gpt/function-calling">here</a>.</p>
<h2>The future of LLM-powered agents is bright!</h2>
<p>Large language models have been one of the most significant advances of the past decade. Capable of reasoning and talking like a human, they appear to be able to do anything. Despite this, several engineering challenges arise in building around an LLM, such as context limits, reasoning, and long-term retention.</p>
<p>Using the methods described above, <strong>AgentGPT</strong> unlocks the full potential of powerful models such as GPT-4. <em>We can give any model superpowers using novel prompting methods, efficient vector databases, and abundant API tools</em>. It&#x27;s only the start, and we hope you&#x27;ll join us on this journey.</p>
<h2>Conclusion</h2>
<p>AgentGPT represents a powerful approach to building AI agents that reason, remember, and perform. By leveraging prompt
engineering, vector databases, and API tools, we can overcome the limitations of standalone LLMs and create agents that
demonstrate agentic behavior.</p>
<p>With the ability to reason, plan, and reflect, AgentGPT agents can tackle complex tasks and interact with the
environment in a meaningful way. By incorporating long-term memory through vector databases and utilizing APIs, we
provide agents with access to a vast pool of knowledge and resources.</p>
<p>AgentGPT is a step towards unlocking the full potential of LLMs and creating intelligent agents that can assist and
collaborate with humans in various domains. The combination of language models, prompt engineering, external memory,
and API interactions opens up exciting possibilities for AI agents in the future.</p>
<h2>Extra Resources</h2>
<p>Are you interested in learning more about prompt engineering? We encourage you to check out other informational posts on our site, or you can check out the fantastic places below, or if you are interested in contributing, check out our <a href="https://github.com/reworkd/AgentGPT">GitHub repo</a>.</p></div></div></div></div></div><footer class="flex flex-col items-center justify-center gap-2 pb-2 sm:gap-4 sm:pb-4 lg:flex-row"><div class="hidden cursor-pointer flex-row justify-center space-x-4 lg:flex"><a href="https://www.ycombinator.com/companies/reworkd/jobs" target="_blank" rel="noopener noreferrer" class="group w-full rounded-full bg-transparent px-2 text-sm text-white/50 transition-colors duration-300 ease-in-out hover:text-white/90">Careers</a><a href="https://status.reworkd.ai" target="_blank" rel="noopener noreferrer" class="group w-full rounded-full bg-transparent px-2 text-sm text-white/50 transition-colors duration-300 ease-in-out hover:text-white/90"><div class="flex items-center gap-3"><p>Status</p><div class="h-[6px] w-[6px] animate-pulse items-center justify-center rounded-full bg-green-500 ring-[3px] ring-green-500 ring-opacity-60"></div></div></a><a href="https://agentgpt.reworkd.ai/privacypolicy" target="_blank" rel="noopener noreferrer" class="group w-full rounded-full bg-transparent px-2 text-sm text-white/50 transition-colors duration-300 ease-in-out hover:text-white/90">Privacy</a><a href="https://agentgpt.reworkd.ai/terms" target="_blank" rel="noopener noreferrer" class="group w-full rounded-full bg-transparent px-2 text-sm text-white/50 transition-colors duration-300 ease-in-out hover:text-white/90">Terms</a></div><div class="font-inter text-xs font-normal text-gray-300 sm:text-sm lg:order-first">Â© 2023 Reworkd AI, Inc.</div></footer></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json" crossorigin="">{"props":{"pageProps":{"postData":{"slug":"Understanding-AgentGPT","title":"Understanding AgentGPT: How we build AI agents that reason, remember, and perform.","description":"How we build AI agents that reason, remember, and perform.","imageUrl":"https://petal-diplodocus-04a.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fef520689-ca1b-4489-98aa-41136f565840%2FCybrCo_Art_human-like_robot_typing_on_a_computer_in_a_dark_room_a0174b88-a5b9-4b82-98c6-734dbbde8d09.webp?id=f768fec9-bd6a-43ae-811d-1adb065c6c8e\u0026table=block\u0026spaceId=46c3481b-d8de-4c34-8647-2292d63a5f29\u0026width=2000\u0026userId=\u0026cache=v2","date":"July 17th, 2023","datetime":"2023-07-17","category":{"title":"Tech","href":"#"},"author":{"name":"Arthur Riechert","role":"Writer","href":"#","imageUrl":"https://pbs.twimg.com/profile_images/1676828916546248704/5YMDlr1U_400x400.jpg"},"content":"\r\n![CybrCo_Art_human-like_robot_typing_on_a_computer_in_a_dark_room_a0174b88-a5b9-4b82-98c6-734dbbde8d09.webp](https://petal-diplodocus-04a.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fef520689-ca1b-4489-98aa-41136f565840%2FCybrCo_Art_human-like_robot_typing_on_a_computer_in_a_dark_room_a0174b88-a5b9-4b82-98c6-734dbbde8d09.webp?id=f768fec9-bd6a-43ae-811d-1adb065c6c8e\u0026table=block\u0026spaceId=46c3481b-d8de-4c34-8647-2292d63a5f29\u0026width=2000\u0026userId=\u0026cache=v2)\r\n\r\n\u003e Alt: A robotic agent types at a laptop in a dark room.\r\n\r\n---\r\n\r\nThe invention of the **Generative Pre-trained Transformer (GPT)** is one of the recent decade's most important\r\nadvancements in AI technology. The GPTs powering today's **Large Language Models (LLMs)** demonstrate a _remarkable\r\nability for reasoning, understanding, and planning_. However, their true potential has yet to be fully realized.\r\n\r\nAt **Reworkd**, we believe that the _true power of LLMs lies in agentic behavior_. By engineering a system that draws on\r\nLLMs' emergent abilities and providing an ecosystem that supports environmental interactions, we can draw out the full\r\npotential of models like GPT-4. Here's how AgentGPT works.\r\n\r\n## LLMs have a lot of limitations.\r\n\r\nThe main products shipping LLMs are chatbots powered by\r\n\r\n[Foundation Model - Techopedia](https://www.techopedia.com/definition/34826/foundation-model).\r\n\r\nIf you have any familiarity working with OpenAI's API, a common formula you might use for chatting with the model may\r\ninclude:\r\n\r\n- Taking the user's message.\r\n- Adding a list of chat histories.\r\n- Sending the chat history across the API to retrieve a completion.\r\n\r\nThis method works fine when the scope of conversations is small; however, _as you continue adding new messages to the\r\nchat history, the size and complexity of completions balloons_, and you will quickly run into a wall: the dreaded\r\ncontext limit.\r\n\r\nA **context limit** is the maximum number of **tokens** (a token usually represents a single word) that can be input\r\ninto\r\nthe model for a single response. They are necessary because the _computational cost as we add additional tokens tends to\r\nincrease quadratically_. However, they are often the bane of prompt engineers.\r\n\r\nOne solution is to measure the number of tokens in the chat history before sending it to the model and removing old\r\nmessages to ensure it fits the token limit. While this approach works, it ultimately reduces the amount of knowledge\r\navailable to the assistant.\r\n\r\nAnother issue that standalone LLMs face is the need for human guidance. Fundamentally, LLMs are next-word predictors,\r\nand often, their internal structure is not inherently suited to higher-order thought processes, such as **reasoning**\r\nthrough complex tasks. This weakness doesn't mean they can't or don't reason. In fact, there are several [studies](https://arxiv.org/abs/2205.11916#:~:text=While%20these%20successes%20are%20often%20attributed%20to%20LLMs%27,%22Let%27s%20think%20step%20by%20step%22%20before%20each%20answer.) that shows they can. However, it does mean they face certain impediments. For example, the LLM itself can create a logical list of steps; however, it has _no built-in mechanisms for observation and reflection on that list._\r\n\r\nA pre-trained model is essentially a \"black box\" for the end user in which the final product that is shipped has\r\n_limited to no capability of actively updating its knowledge base and tends to act in unpredictable ways_. As a result,\r\nit's [hallucination](https://arxiv.org/abs/2202.03629)-prone.\r\n\r\nThus, it requires a lot of effort on the user's part to guide the model's output, and prompting the LLM itself becomes a\r\njob on its own. This extra work is a far cry from our vision of an AI-powered future.\r\n\r\nBy providing a platform to give LLMs agentic abilities, _AgentGPT aims to overcome the limitations of standalone LLMs by\r\nleveraging prompt engineering techniques, vector databases, and API tooling._ Hereâ€™s some interesting work that is being\r\ndone with the agent concept:\r\n\r\n[![Tweet by Dr. Jim Fan](https://platform.twitter.com/embed/Tweet.html?dnt=false\u0026embedId=twitter-widget-0\u0026frame=false\u0026hideCard=false\u0026hideThread=false\u0026id=1673006745067847683\u0026lang=en\u0026origin=https%3A%2F%2Fpublish.twitter.com%2F%3Fquery%3Dhttps3A2F2Ftwitter.com2FDrJimFan2Fstatus2F1673006745067847683%26widget%3DTweet\u0026theme=light\u0026widgetsVersion=82e1070%3A1619632193066\u0026width=550px)](https://twitter.com/DrJimFan/status/1673006745067847683)\r\n\r\n\u003e Alt: A Twitter post by Dr. Jim Fan\r\n\r\n## What are agents?\r\n\r\nIn a general sense, [agents](https://zapier.com/blog/ai-agent/) are rational actors. They use thinking and reasoning to\r\ninfluence their environment. _This could be in the form of solving problems or pursuing specific goals. They might\r\ninteract with humans or utilize tools._ Ultimately, we can apply this concept to LLMs to instill more intelligent and\r\nlogical behavior.\r\n\r\nIn AgentGPT, large language models essentially function as the **brain** of each agent. As a result, we can produce\r\npowerful agents by cleverly _manipulating the English language_ and engineering a _framework that supports\r\ninteroperability between LLM completions and a diverse set of APIs_.\r\n\r\n### Engineering this system consists of 3 parts.\r\n\r\n**Reasoning and Planning.** If you were to simply take a general goal, such as \"build a scaling e-commerce platform,\"\r\nand\r\ngive it to ChatGPT, you would likely get a response along the lines of \"As an AI language modelâ€¦.\" However, through\r\n**prompt engineering**, we can get a model to _break down goals into digestible steps and reflect on them_ with a method\r\ncalled chain of thought prompting.\r\n\r\n**Memory.** When dealing with memory, we divide the problem into **short-term** and **long-term**. In managing\r\nshort-term\r\nmemory, we can use prompting techniques such as _few-shot prompting to steer LLM responses_. However, _cost and context\r\nlimits make it tricky to generate completions without limiting the breadth of information_ a model can use to make\r\ndecisions.\r\n\r\nSimilarly, this issue also arises in **long-term memory** because it would be impossible to provide an appropriate\r\ncorpus\r\nof writing to bridge the gap between GPT -4's cutoff date, 2021, till today. By using vector databases, we attempt to\r\novercome this using specialized models for _information retrieval in high-dimensional vector spaces_.\r\n\r\n**Tools**. Another challenge in using LLMs as general actors is their confinement to text outputs. Again, we can use\r\nprompt engineering techniques to solve this issue. We can generate predictable function calls from the LLM through\r\nfew-shot and chain-of-thought methods, utilizing API tools like **Google Search**, **Hugging Face**, **Dall-E**, etc. In\r\naddition, we can use fine-tuned LLMs that only return responses in specialized formatting, like JSON. This is the\r\napproach OpenAI took when they recently released the function calling feature for their API.\r\n\r\nThese three concepts have formed the backbone of multiple successful agent-based LLM platforms such\r\nas [Microsoft Jarvis](https://github.com/microsoft/JARVIS), [AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT), [BabyAGI](https://github.com/yoheinakajima/babyagi),\r\nand of course, AgentGPT. With this brief overview in mind, let's dive deeper into each component.\r\n\r\n## How do we get agents to act intelligently?\r\n\r\n**Prompt engineering** has become highly popularized, and it's only natural given its ability to _increase the\r\nreliability of LLM responses_, opening a wide avenue of potential applications for generative AI. AgentGPT's ability to\r\nthink and reason is a result of novel prompting methods.\r\n\r\n### A Brief Intro to Prompt Engineering\r\n\r\nPrompt engineering is a largely empirical field that aims to find methods to steer LLM responses by finding clever ways\r\nto use the English language. _You can think of it like lawyering, where every nuance in the wording of a prompt counts._\r\n\r\nThese are the main concepts and building blocks for more advanced prompting techniques:\r\n\r\n1. **Zero-Shot** involves sending the raw command directly to the LLM with little to no formatting.\r\n2. **Few-Shot** gives context for completions in the form of example responses.\r\n3. **Chain-of-Thought** guides the model in reasoning through generating and reasoning over a complex task.\r\n\r\n### How AgentGPT Uses Prompt Engineering\r\n\r\nAgentGPT uses an advanced form of chain-of-thought prompting called **Plan-and-Solve** to generate the steps you see\r\nwhen\r\noperating the agents.\r\n\r\nTraditionally, chain-of-thought prompting utilized few-shot techniques to provide examples of a thinking and reasoning\r\nprocess. However, as is becomes a theme, it becomes more costly as the complexity of a task increases because we will\r\nneed to provide more context.\r\n\r\n**Plan-and-solve (PS):** By virtue of being a zero-shot method, it provides a _prompting framework for LLM-guided\r\nreasoning using \"trigger\" words_. These keywords trigger a reasoning response from the model.\r\n\r\nWe can expand on this concept by _modifying the prompt to extract important variables and steps to generate a final\r\nresponse with a cohesive format_. This method allows us to parse the final response and display it for the end user as\r\nwell as feed sub-steps into future plan-and-solve prompts.\r\n\r\n![Screen Shot 2023-07-01 at 12.25.37 PM.png](https://petal-diplodocus-04a.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F29d8c98c-21e6-4991-992d-62d95fd40dba%2FScreen_Shot_2023-07-01_at_12.25.37_PM.png?id=021895a6-149a-4282-aa8e-6719e7d7c47a\u0026table=block\u0026spaceId=46c3481b-d8de-4c34-8647-2292d63a5f29\u0026width=2000\u0026userId=\u0026cache=v2)\r\n\r\n\u003e Alt: Picture of Plan \u0026 Solve\r\n\r\nWhile PS prompting helps evoke a reasoning response, it still misses a fundamental concept in reasoning, and that is\r\nproper handling for reflection and action. **Reflection**is _fundamental for any agent because it must rationalize an\r\naction, perform that action, and use feedback to adjust future actions._ Without it, the agent would be stateless and\r\nunchanging.\r\n\r\nAgentGPT uses a prompting framework called Reasoning and Acting ([ReAct](https://arxiv.org/pdf/2210.03629.pdf)) to\r\nexpand on the capabilities of the Plan-and-Solve concept. **ReAct** aims to _enable a framework for the model to access\r\nfresh knowledge through external knowledge bases and make observations of actions it has taken_. Using those\r\nobservations, the LLM can make educated decisions on the next set of steps to complete while performing actions to query\r\nknowledge bases such as **Google Search** or **Wikipedia API**.\r\n\r\nPrompt engineering is largely effective in resolving challenges in short-term memory as well as instilling the reasoning\r\nbehavior that you can see when AgentGPT is at work. However, prompt engineering does not resolve the issue of long-term\r\nmemory. This issue is where vector databases come in, and we will look at those next.\r\n\r\n![Screen Shot 2023-07-03 at 3.12.56 AM.png](https://petal-diplodocus-04a.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F481f0812-00e5-4cb1-9ed6-4f2f9215eef5%2FScreen_Shot_2023-07-03_at_3.12.56_AM.png?id=8002f409-2913-4e68-b8b6-6100c4128cf5\u0026table=block\u0026spaceId=46c3481b-d8de-4c34-8647-2292d63a5f29\u0026width=2000\u0026userId=\u0026cache=v2)\r\n\r\n\u003e Alt : ReAct (Reason + Act) Logic Picture\r\n\r\n\u003e The ReAct framework allows us to generate a reasoning response, an action, and a reflection to\r\n\u003e steer the modelâ€™s response. This example is courtesy of the following\r\n\u003e paper: [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)\\*\r\n\r\n## How do we give agents a working memory?\r\n\r\nWhile we have seen that _prompt engineering is largely effective in resolving issues with short-term memory and\r\nreasoning_, we cannot solve long-term memory solely through clever English. Since we are not allowed to update the model\r\nto learn our data, we must build an external system for storing and retrieving knowledge.\r\n\r\nA clever solution might use an LLM to _generate summaries of previous conversations as context for the prompt_. However,\r\nthere are three significant issues with this. First, we are diluting the relevant information for the conversation;\r\nsecond, it introduces another cost area by paying for API usage for those summaries; and third, it's unscalable.\r\n\r\nThus, prompts appear to be ineffective for long-term memory. Seeing as _long-term memory is a problem of storage and\r\nefficient retrieval of information_, there is no absence of research in the study of search, so we must look towards\r\nvector databases.\r\n\r\n### Vector Databases Demystified\r\n\r\n**[Vector databases](https://aws.amazon.com/what-is/vector-databases/)** have been hyped up for a while now, and the\r\nhype\r\nis very deserved. They are an efficient way of storing and retrieving vectors by allowing us to use some fun new\r\n_algorithms to query billions - even trillions - of data records in milliseconds._\r\n\r\nLet's start with a little bit of vocabulary:\r\n\r\n- A **vector** in the context of an LLM is a representation of a piece of text that a model like GPT-4 encodes.\r\n- A **vector space** contains many of these vectors.\r\n- An **embedding** is the vectorized version of a text.\r\n\r\n### Vector libraries like\r\n\r\n[Facebook AI Similarity Search](https://www.bing.com/ck/a?!\u0026\u0026p=a0f4167bc6cd7db9JmltdHM9MTY4ODM0MjQwMCZpZ3VpZD0zOTYwYjczZS1hNzg2LTY5Y2MtMjM2YS1hNDdmYTYwMjY4MjImaW5zaWQ9NTIwMQ\u0026ptn=3\u0026hsh=3\u0026fclid=3960b73e-a786-69cc-236a-a47fa6026822\u0026psq=faiss+github\u0026u=a1aHR0cHM6Ly9naXRodWIuY29tL2ZhY2Vib29rcmVzZWFyY2gvZmFpc3M\u0026ntb=1) (\r\nFAISS) give us access to valuable _tools to control these vectors and locate them efficiently in the vector space._\r\n\r\nSince the text is in a numerical embedding dictated by the model type (i.e., text-embedding-ada-002), there is some\r\nlocation in space that the text exists in, and it's based on the numbers that compose its vector. That means _similar\r\ntexts will be represented as vectors with similar numbers, and thus, they will likely be grouped closely. On the other\r\nhand, less similar texts will be further away_. For example, texts about cooking will be closer to food than texts about\r\nphysics.\r\n\r\nThere are several different algorithms for querying the vector space, but the most relevant to this discussion is the\r\ncosine similarity search. **[Cosine similarity](https://www.geeksforgeeks.org/cosine-similarity/)** measures the cosine\r\nof the angle between two non-zero vectors. _It is a measure of orientation, meaning that it's used to determine how\r\nsimilar two documents (or whatever the vectors represent) are_. Cosine similarity can range from -1 to 1, with -1\r\nmeaning the vectors are diametrically opposed (completely opposite), 0 meaning the vectors are orthogonal (or\r\nunrelated), and 1 meaning the vectors are identical.\r\n\r\nFAISS is helpful in managing these vector spaces, but it is not a database. _Vector libraries\r\nlack [CRUD](https://www.freecodecamp.org/news/crud-operations-explained/) operations, which makes them alone unviable\r\nfor long-term memory_, and that's where cloud services such as Pinecone and Weaviate step in.\r\n\r\n**Pinecone** and **Weaviate** essentially do all the hard work of managing our vectors. They provide an API that allows\r\nyou\r\nto upload embeddings, perform various types of searches, and store those vectors for later. _They provide the typical\r\nCRUD functions we need to instill memory into LLMs in easily-accessible Python modules._\r\n\r\nBy using them, we can encode large amounts of information for future storage and retrieval. For instance, when the LLM\r\nneeds extra knowledge to complete a task, we can prompt it to query the vector space to find relevant information. Thus,\r\nwe can create long-term memory.\r\n\r\n![CybrCo_Art_A_human-like_robot_touching_a_flower_for_the_first_t_92e97d56-54fa-4bb0-8581-5a1e15fd94aa.webp](https://petal-diplodocus-04a.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fad2521b3-1c6b-4f16-b719-d2b766570c61%2FCybrCo_Art_A_human-like_robot_touching_a_flower_for_the_first_t_92e97d56-54fa-4bb0-8581-5a1e15fd94aa.webp?id=8d261d10-f4e4-4798-bc33-8f40da67bb42\u0026table=block\u0026spaceId=46c3481b-d8de-4c34-8647-2292d63a5f29\u0026width=2000\u0026userId=\u0026cache=v2)\r\n\r\n\u003e Alt : Robot With A Rose In Hand\r\n\r\n## Tools to interact with the environment\r\n\r\nWhile **prompt engineering** and **vector databases** resolve many of the limitations and challenges of LLMs, there is\r\nstill the problem of agent interaction. _How can we extend the capabilities of an LLM to interact with the environment\r\noutside of text?_\r\n\r\nAPIs are the answer. By utilizing APIs, we can give our agents the ability to perform a wide range of actions and\r\naccess external resources.\r\n\r\nHere are a few examples:\r\n\r\n- **Google Search API**: Allows agents to search the web and retrieve relevant information.\r\n- **Hugging Face**: Provides access to various NLP models and transformers for tasks such as summarization, translation,\r\n  sentiment analysis, and more.\r\n- **Dall-E**: Enables agents to generate images from textual descriptions.\r\n- **OpenAI's GPT API**: Allows agents to utilize the GPT-4 model for text completion and generation.\r\n\r\nUsing API tools in combination with prompt engineering techniques, we can create prompts that generate predictable\r\nfunction calls and utilize the output of API requests to enhance the agent's capabilities. This enables agents to\r\ninteract with the environment in a meaningful way beyond text-based interactions.\r\n\r\n### Engineering Robust Function Calls\r\n\r\nAgain, we can achieve tooling through prompt engineering by _representing the tool we want to provide for the model_ as a **function**. _We can then tell the model that this function exists in a prompt, so our program can call it programmatically based on the model's response_. First, however, we should examine the main challenges in implementing tool interactions: consistency, context, and format.\r\n\r\nFor example, responses tend to vary among chat completions that use the same prompt. Thus, getting the LLM to issue a function call consistently is challenging. A minor solution may include adjusting the **temperature** of the model (a parameter to control the randomness), but the best solution should leverage an LLM's reasoning abilities. Thus, _we can use the ReAct framework to help the llm understand when to issue function calls._\r\n\r\nIn doing this, we will still run into another major issue. How will the LLMs understand what tools are at their disposal? We could include the available tools in a prompt, but this could significantly increase the number of tokens we would need to send to the model. While this may be fine for an application that runs on a couple of tools, it will increase costs as we add more tools to the system. Thus, _we would use vector databases to help the LLM look up relevant tools it needs._\r\n\r\nFinally, we need to generate function calls in a predictable format. This format should include provisions for the name of the function and the parameters it takes, and it must include delimiters that allow us to parse and execute the response for those parameters programmatically. _For instance, you can prompt the model to only return responses in JSON and then use built-in Python libraries to parse the stringified JSON._\r\n\r\nRecently, it became even easier to use this type of method as well. In late June, OpenAI released **gpt-4-0613** and **gpt-3.5-turbo-16k-0613** (whew, these names are getting long). They natively support function calls by using a model fine-tuned for JSON to return easy-to-use function calls. You can read more about it [here](https://platform.openai.com/docs/guides/gpt/function-calling).\r\n\r\n## The future of LLM-powered agents is bright!\r\n\r\nLarge language models have been one of the most significant advances of the past decade. Capable of reasoning and talking like a human, they appear to be able to do anything. Despite this, several engineering challenges arise in building around an LLM, such as context limits, reasoning, and long-term retention.\r\n\r\nUsing the methods described above, **AgentGPT** unlocks the full potential of powerful models such as GPT-4. _We can give any model superpowers using novel prompting methods, efficient vector databases, and abundant API tools_. It's only the start, and we hope you'll join us on this journey.\r\n\r\n## Conclusion\r\n\r\nAgentGPT represents a powerful approach to building AI agents that reason, remember, and perform. By leveraging prompt\r\nengineering, vector databases, and API tools, we can overcome the limitations of standalone LLMs and create agents that\r\ndemonstrate agentic behavior.\r\n\r\nWith the ability to reason, plan, and reflect, AgentGPT agents can tackle complex tasks and interact with the\r\nenvironment in a meaningful way. By incorporating long-term memory through vector databases and utilizing APIs, we\r\nprovide agents with access to a vast pool of knowledge and resources.\r\n\r\nAgentGPT is a step towards unlocking the full potential of LLMs and creating intelligent agents that can assist and\r\ncollaborate with humans in various domains. The combination of language models, prompt engineering, external memory,\r\nand API interactions opens up exciting possibilities for AI agents in the future.\r\n\r\n## Extra Resources\r\n\r\nAre you interested in learning more about prompt engineering? We encourage you to check out other informational posts on our site, or you can check out the fantastic places below, or if you are interested in contributing, check out our [GitHub repo](https://github.com/reworkd/AgentGPT).\r\n"}},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"Understanding-AgentGPT"},"buildId":"klp32T_lMBqz5AWL-jk-X","isFallback":false,"gsp":true,"locale":"en","locales":["en","hu","fr","de","it","ja","lt","zh","zhtw","ko","pl","pt","ro","ru","uk","es","nl","sk","hr","tr"],"defaultLocale":"en","scriptLoader":[]}</script></body></html>